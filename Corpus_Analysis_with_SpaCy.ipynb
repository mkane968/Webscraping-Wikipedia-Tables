{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmGbAeU+of/kAi8ZkxPw/3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Webscraping-Wikipedia-Tables/blob/main/Corpus_Analysis_with_SpaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corpus Analysis with SpaCy\n",
        "##Introduction\n",
        "\n",
        "This tutorial describes how to conduct cleaning and text analysis on a corpus of texts using SpaCy. It will be of interest to researchers who want to curate texts for analysis and perform lemmatization, part-of-speech tagging, and other text enrichment tasks to help answer their research questions. \n",
        "\n",
        "###Why Use SpaCy for Text Analysis? \n",
        "\n",
        "SpaCy is an industrial-strength library for natural language processing. One of its primary usages is to retrieve a variety of linguistic annotations from a text or corpus (e.g. lemmas, part of speech tags, named entities), so it's valuable for researchers who want to know more about the grammatical structure of their corpora. \n",
        "\n",
        "While there are several Python libraries that can conduct similar text-mining tasks, SpaCy holds the following advantages: \n",
        "*   It's **fast and simple to set up and call the nlp pipeline**; no need to call a wide range of packages and functions for each individual task (SOURCE)\n",
        "*   It **uses only the \"latest and best\" algorithms** for text-processing tasks, so it's easy to run and kept up-to-date by the developers (SOURCE)\n",
        "*   It's proven to **perform better on text-splitting tasks** than NLTK, since it constructs syntactic trees for each sentence it is called on (SOURCE)\n",
        "\n",
        "###Prerequisites\n",
        "\n",
        "You will need access to the following materials and platforms to complete this tutorial: \n",
        "*   **Google Colaboratory**: A Google platform which allows you to run Python in a web browser. Access is free with a Google account. Get started with Colab here: https://colab.research.google.com/ \n",
        "*   **A corpus of plain text files** on which you wish to perform analysis. A sample corpus from Project Gutenberg can be accessed [here.](https://github.com/mkane968/Corpus-Analysis-with-SpaCy/tree/main/data)\n",
        "\n",
        "Though there are no other Programming Historian tutorial that specifically incorporate SpaCy analysis, this notebook will build on other text-mining guides available on the platform, including: \n",
        "\n",
        "*  [Corpus Analysis with Antconc (Froelich, 2015)](https://programminghistorian.org/en/lessons/corpus-analysis-with-antconc#collocates-and-word-lists) - Provides robust explanation of processing and exploring text corpora; while this tutorial focuses on keywords-in-context, SpaCy retrieves part-of-speech tags, named entities, and other named entities that can provide additional insight to researchers\n",
        "\n",
        "*   [Analyzing Documents with TF-IDF (Lavin, 2019)](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf#how-the-algorithm-works) - Describes the process of conducting TF-IDF analysis which can be enriched with the stemming and lemmatization processes that spaCy enables\n",
        "\n",
        "\n",
        "###Tutorial Goals\n",
        "By the end of this tutorial, you will be able to: \n",
        "*   Upload a corpus of texts to Google Colab\n",
        "*   Clean the corpus by lowercasing, removing stop words and removing punctuation \n",
        "*   Enrich the corpus through lemmatization, chunking,  part-of-speech tagging, and named entity recognition using SpaCy\n",
        "\n",
        "###Table of Contents: \n",
        "1. Install Packages \n",
        "2. Load Text Files into DataFrame\n",
        "3. Cleaning and Tokenization\n",
        "4. Text Enrichment (Lemmatization, Part of Speech Tagging, Named Entity Recognition)"
      ],
      "metadata": {
        "id": "4wnznsMOACSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Install Packages\n",
        "\n",
        "Import spaCy and related libraries and packages. It is common practice to do this at the very top of the file instead of interspersing them with your code to improve efficiency. These packages can be run in a single cell of code; below, the markdown text describes how each downloaded package or library will be used in the analysis. \n"
      ],
      "metadata": {
        "id": "wVeD4Ik7D43F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xrm0CzvO_Uhw"
      },
      "outputs": [],
      "source": [
        "#Imports spaCy itself, necessary to use features \n",
        "#!pip install spaCy\n",
        "import spacy\n",
        "#Load the natural language processing pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#Load spaCy visualizer\n",
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Load Text Files into DataFrame\n",
        "\n",
        "After all necessary packages have been installed, it is time to upload the texts for analysis. The key here is to read the texts into Google Colab in a way that will make them recognizable for analysis. Run the following code to “mount” the Google Drive, which allows your Google Colab notebook to access any files on your Drive. A box will pop up asking for permission for the notebook to access your Drive files; click “Connect to Google Drive,” select Google account to connect to, and click “Allow.” "
      ],
      "metadata": {
        "id": "zQ8ve667EvxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Oj5Ufz8xE7qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, load the files for analysis into your Google Drive. To complete this step, you must have the files of interest saved in a folder on your local machine. Once you run this line of code, a button will pop up directing you to “Choose Files” – click the button and a file explorer box will pop up. From here, navigate to the folder where your files are stored, select the files of interest, and click “Open.” The files will then be uploaded to your Google Drive; you will see the upload complete as output of your cell and can access the files by clicking the file icon in the bar on the left-hand side of the notebook.\n"
      ],
      "metadata": {
        "id": "0FqwCyl8gtNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Selet multiple files to upload from local folder\n",
        "from google.colab import files\n",
        "\n",
        "uploaded_files = files.upload()\n"
      ],
      "metadata": {
        "id": "XaVUPnFIE_kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have files upon which we can perform analysis. To check what form of data we are working with, use the type() function. It should return that your files are contained in a dictionary, where keys are the file names and values are the content of each file. \n"
      ],
      "metadata": {
        "id": "pg0w6jIEgxlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(uploaded_files)"
      ],
      "metadata": {
        "id": "N3f8cxLrgzUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we’ll make the data easier to manage by inserting it into a Pandas dataframe. This will organize the texts into a table of rows and columns–in this case, the first column will contain the names of the files, and the second column will contain the context of each file. Since the files are currently stored in a dictionary, use the DataFrame.from_dict() function to append them to a new dataframe.\n"
      ],
      "metadata": {
        "id": "fmYyZTzog32k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add files into dataframe\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame.from_dict(uploaded_files, orient='index')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "s2w09XuhKqOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here, you can reset the index (the very first row of the dataframe). This will make data wrangling easier later.  "
      ],
      "metadata": {
        "id": "yr-yzcsng6o4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reset index and add column names to make wrangling easier\n",
        "df = df.reset_index()\n",
        "df.columns = [\"Title\", \"Text\"]\n",
        "df"
      ],
      "metadata": {
        "id": "BJJPgl5FL9qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The texts in your resulting dataframe are now ready for cleaning and analysis. \n"
      ],
      "metadata": {
        "id": "IHOPW8N_g9B_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Cleaning and Tokenization\n",
        "\n",
        "From a quick scan of the dataframe, it is evident that some preliminary cleaning is required. First use the .decode() module to remove any utf-8 characters embedded in the texts (b'\\xef\\xbb\\xbf). It is also important to remove newline characters (\\n, \\r) through a simple string replacement line. These are NOT functions of spaCy but are necessary to make the code recognizable for further cleaning and tokenization. \n"
      ],
      "metadata": {
        "id": "Il5slp5CMKeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove encoding characters from Text column (b'\\xef\\xbb\\xbf)\n",
        "df['Text'] = df['Text'].apply(lambda x: x.decode('utf-8', errors='ignore'))\n",
        "df.head()\n",
        "\n",
        "#Remove newline characters\n",
        "df['Text'] = df['Text'].str.replace(r'\\s+|\\\\r', ' ', regex=True) \n",
        "df['Text'] = df['Text'].str.replace(r'\\s+|\\\\n', ' ', regex=True) \n",
        "df.head()"
      ],
      "metadata": {
        "id": "buepJxsC-wRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next, most basic operation to perform is lowercasing all tokens in the texts. This will prevent incorrect calculations in later case-sensitive analysis; for example, if lowercasing is not performed, “House” and “house” may be counted as two different words. "
      ],
      "metadata": {
        "id": "xxBqzHdehT7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lowercase all words\n",
        "df['Text'] = df['Text'].str.lower()\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "s8iYmAYsENde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process used to split up full text into smaller parts for analysis. SpaCy has a built-in function for tokenization that involves segmenting texts into individual parts like words and punctuation. Take the example of an individual sentence: "
      ],
      "metadata": {
        "id": "CIZWFJ5thWZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"This is 'an' example? sentence\")\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "id": "AYx6IjM_ZiU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What this function is doing is calling the nlp pipeline, which contains the data and components needed for text processing. When the nlp pipeline is called on a sentence, it splits that sentence on each whitespace and reviews its components. Components are then split based on rules for words, punctuations, prefixes, suffixes, etc. Each token is then loaded into a new object that we’ve called “doc.” Calling nlp also enables part of speech tagging, lemmatization, and other enrichment procedures we’ll discuss further below. \n",
        "\n",
        "Since we are working with multiple long texts, we are going to use nlp.pipe, which processes batches of texts as doc objects. Here we’ll tokenize each text in our dataframe, append each set of tokens to a list, and add the new token lists to a new column in the dataframe.\n"
      ],
      "metadata": {
        "id": "p5LzNhX6ha97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize with spaCy\n",
        "\n",
        "#Create list for tokens\n",
        "token_list = []\n",
        "\n",
        "# Disable POS, Dependency Parser, and NER since all we want is tokenizer \n",
        "with nlp.disable_pipes('tagger', 'parser', 'ner'):\n",
        "  #Iterate through each doc object (each text in dataframe) and tokenize, append tokens to list\n",
        "    for doc in nlp.pipe(df.Text.astype('unicode').values, batch_size=100):\n",
        "        word_list = []\n",
        "        for token in doc:\n",
        "            word_list.append(token.text)\n",
        "\n",
        "        token_list.append(word_list)\n",
        "        \n",
        "#Make token list a new column in dataframe\n",
        "df['token_list'] = token_list\n",
        "\n",
        "#Check token list\n",
        "df.head()"
      ],
      "metadata": {
        "id": "av2FqJD5HAE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When tokenizing texts, you can also exclude stopwords. Stopwords are words which may hold little significance to text analysis, such as very common words like “the” or “and.” SpaCy has a built-in dictionary of stopwords which you can access. You can also add or remove your own stopwords, as shown below:"
      ],
      "metadata": {
        "id": "FUQF0upEhdm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding and removing stopwords to default list\n",
        "#See list of default stopwords\n",
        "print(nlp.Defaults.stop_words)\n",
        "\n",
        "#Remove a  stopword\n",
        "nlp.Defaults.stop_words.remove(\"becomes\")\n",
        "\n",
        "#Add stopword\n",
        "nlp.Defaults.stop_words.add(\"book\")\n",
        "\n",
        "#Check updated list of default stopwords\n",
        "print(nlp.Defaults.stop_words)"
      ],
      "metadata": {
        "id": "lBveRLWFSASN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To tokenize texts without stopwords, follow the same process above using nlp.pipe, but only append tokens to list that are NOT included in stopwords list and append these to a new row in the dataframe. \n"
      ],
      "metadata": {
        "id": "Il_t3zO-hflr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove all stopwords and append remaining tokens to new df column\n",
        "token_list_nostops = []\n",
        "\n",
        "# Disable POS, Dependency Parser, and NER since all we want is tokenizer \n",
        "with nlp.disable_pipes('tagger', 'parser', 'ner'):\n",
        "  #Iterate through each doc object (each text in dataframe) and tokenize, append tokens to list\n",
        "    for doc in nlp.pipe(df.Text.astype('unicode').values, batch_size=100):\n",
        "        nostops_word_list = []\n",
        "        for token in doc:\n",
        "            if token.text not in nlp.Defaults.stop_words:\n",
        "              nostops_word_list.append(token.text)\n",
        "\n",
        "        token_list_nostops.append(nostops_word_list)\n",
        "\n",
        "#Make token list a new column in dataframe\n",
        "df['token_list_nostops'] = token_list_nostops\n",
        "\n",
        "#Check list of tokens without stopwords\n",
        "df.head()"
      ],
      "metadata": {
        "id": "8zyfrC5FS_bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on the goals of your analysis, you may want to remove or keep stopwords. One case where stopword removal may be useful is if you want to compare document similarity. SpaCy calculates document similarity based on corpus word vectors; since stopwords are words that appear throughout texts, they will heighten document similarity scores even if their content is very different. Observe the difference here: \n"
      ],
      "metadata": {
        "id": "HOGEZ1GUhiZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Stopwords Test Case - Word Vector Similarity\n",
        "#Load a larger pipeline with vectors\n",
        "#!spacy download en_core_web_md\n",
        "#nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Compare similarity between two documents without stopwords\n",
        "doc1 = nlp(df.Tokens[0])\n",
        "doc2 = nlp(df.Tokens[2])\n",
        "print(f'The similarity between ' + str(df.Title[0]) + ' and ' + str(df.Title[2]) + ' with stopwords is ' + str(doc1.similarity(doc2)))\n",
        "\n",
        "# Compare similarity between two documents without stopwords\n",
        "doc1 = nlp(df.Stop_Tokens[0])\n",
        "doc2 = nlp(df.Stop_Tokens[2])\n",
        "print(f'The similarity between ' + str(df.Title[0]) + ' and ' + str(df.Title[2]) + ' without stopwords is ' + str(doc1.similarity(doc2)))\n"
      ],
      "metadata": {
        "id": "gZF8YTdmelu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopword removal is also useful for topic modeling and classification tasks, where finding general themes across documents is the goal. However, other types of analysis like sentiment analysis are highly sensitive and removing stopwords will change sentence meaning (e.g. removing “not” in the sentence “I was not happy”). When possible, it is recommended to run analysis with and without stopwords and see how the model is impacted. For the rest of this tutorial, we will be using the corpus without stopwords, but you are welcome to replicate analysis with them. \n"
      ],
      "metadata": {
        "id": "M9r1k5BXhtYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Make token_list into a string again for enrichment\n",
        "df['Tokens'] = df['token_list'].str.join(' ')\n",
        "df\n",
        "\n",
        "#Make stoptoken_list a string again for enrichment\n",
        "df['Stop_Tokens'] = df['token_list_nostops'].str.join(' ')\n",
        "df"
      ],
      "metadata": {
        "id": "04mWqVlj046R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Text Enrichment "
      ],
      "metadata": {
        "id": "PcJjGxNrHIFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new dataframe for text enrichment\n",
        "new_df = df[['Tokens', 'Stop_Tokens']].copy()"
      ],
      "metadata": {
        "id": "1xYi2qi3ibLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SpaCy enables several types of text enrichment. We’ll start with lemmatization, which identifies the dictionary root word of each word (e.g. “brighten” for “brightening”). Lemmatization is one of the functions that occurs when the nlp pipe is called; repeat the same process as above to iterate through each document in the dataframe and this time append all lemmas to new column. \n"
      ],
      "metadata": {
        "id": "VuDhGegShusV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get lemmas\n",
        "lemma_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is lemmatization \n",
        "with nlp.disable_pipes('parser', 'ner'):\n",
        "  #Iterate through each doc object and tag lemma, append lemma to list\n",
        "  for doc in nlp.pipe(new_df.Stop_Tokens.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for token in doc:\n",
        "        word_list.append(token.lemma_)\n",
        "        \n",
        "    lemma_list.append(word_list)\n",
        "\n",
        "#Make pos list a new column in dataframe\n",
        "new_df['lemma_list'] = lemma_list\n",
        "\n",
        "#Check lemmas\n",
        "new_df.head()"
      ],
      "metadata": {
        "id": "X7_NQzt-OXJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The nlp pipeline also enables the tagging of each word according to its part of speech. This code will append all parts of speech to a new dataframe column. \n"
      ],
      "metadata": {
        "id": "FG9B5mpFh0-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get part of speech tags\n",
        "pos_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is POS \n",
        "with nlp.disable_pipes('parser', 'ner'):\n",
        "  #Iterate through each doc object and tag POS, append POS to list\n",
        "  for doc in nlp.pipe(df.Text.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for token in doc:\n",
        "        word_list.append(token.pos_)\n",
        "        \n",
        "    pos_list.append(word_list)\n",
        "\n",
        "#Make pos list a new column in dataframe\n",
        "df['pos_list'] = pos_list\n",
        "\n",
        "#Check pos tags\n",
        "df.head()"
      ],
      "metadata": {
        "id": "_kBAmxZK_0Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out the dictionary of SpaCy POS tags [here.](https://machinelearningknowledge.ai/tutorial-on-spacy-part-of-speech-pos-tagging/#:~:text=Spacy%20POS%20Tags%20List,-Every%20token%20is%20assigned%20a) \n",
        "\n",
        "Closely related to POS tagging is dependency parsing, wherein SpaCy identifies how different segments of a text are related to each other. Once the grammatical structure of each sentence is identified, visualizations can be created to show the connections between different words. Since we are working with large texts, our code will break down each text into sentences (spans) and then create dependency visualizers for each span\n"
      ],
      "metadata": {
        "id": "eAeUJXC6h36K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get dependency parsing for single doc\n",
        "doc = nlp(df.Text[0]) \n",
        "print(doc)\n",
        "\n",
        "#Make each sentence a span to break up dependency visualizations\n",
        "spans = doc.sents\n",
        "\n",
        "#Create dependency visualizations \n",
        "displacy.render(spans, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "id": "aHZFtLvJCcU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, SpaCy can tag “named entities” in your text, such as names, dates, organizations, and locations. We’ll again call the nlp pipeline on each document in the corpus and append the named entities to a new column. \n"
      ],
      "metadata": {
        "id": "39utqaRyh_M1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Named Entities\n",
        "ent_list = []\n",
        "\n",
        "with nlp.disable_pipes('tagger', 'parser'):\n",
        "    for doc in nlp.pipe(df.Text.astype('unicode').values, batch_size=100):\n",
        "        ent_list.append(doc.ents)\n",
        "\n",
        "df['ent_list'] = ent_list\n",
        "\n",
        "#Check named entities\n",
        "df.head()"
      ],
      "metadata": {
        "id": "3tlTiFcfJMbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SpaCy also allows you to visualize named entities within single texts, as follows: "
      ],
      "metadata": {
        "id": "xb5FdgvmiBsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get named entities in a single document and visualize\n",
        "doc = nlp(df.Text[0]) \n",
        "\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "kekdvrarS0F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusions\n",
        "Through this tutorial, we've gleaned more information about the grammatical makeup of a text corpus. Such information can be valuable to researchers who are seeking to understand differences between texts in their corpus - for example, *what types of named entities are most common across the corpus? How frequently are certain words used as nouns vs. objects within individual texts and corpora, and what may this reveal about the content or themes of the texts themselves?* \n",
        "\n",
        "SpaCy is also a helpful tool to explore texts without fully-formed research questions in mind; exploring linguistic annotations like those mentioned above can propel further questions and text-mining pipelines, like the following: \n",
        "*   [Getting Started with Topic Modeling and Mallet (Graham, Weingart, and Milligan, 2012)](https://programminghistorian.org/en/lessons/topic-modeling-and-mallet#what-is-topic-modeling-and-for-whom-is-this-useful) - Describes process of conducting topic modeling on a corpora; the SpaCy tutorial can serve as a preliminary step to clean and explore data to be used in topic modeling\n",
        "*   [Sentiment Analysis for Exploratory Data Analysis (Saldaña, 2018)](https://programminghistorian.org/en/lessons/sentiment-analysis#calculate-sentiment-for-a-paragraph) - Describes how to conduct sentiment analysis using NLTK; the SpaCy tutorial provides alternative methods of pre-processing and exploration of entities that may become relevant in sentiment analysis \n",
        "\n"
      ],
      "metadata": {
        "id": "MaZ_fHSCiawV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Official PH Proposal \n",
        "\n",
        "##About You\n",
        "1. Megan Kane\n",
        "2. megan.kane@temple.edu\n",
        "\n",
        "## Lesson Metadata\n",
        "3. Submission Language: English \n",
        "4. Proposed Lesson Title: Corpus Analysis with SpaCy\n",
        "5. Lesson Abstract (3-4 sentences): This lesson will enable users to perform basic text analysis on a textual corpus using SpaCy in Google Colab. It will walk through the process of uploading a corpus to Google Colab, then describe tokenization and cleaning techniques such as stopword and punctuation removal. It will then walk through various text enrichment techniques  using Spacy including lemmatization, part-of-speech tagging, dependency parsing, and named entity recognition.\n",
        "6. Case Study Description (details about your historical example problem) ???\n",
        "7. Learning Outcomes (between 2-3): \n",
        "Prepare any text or corpus for text analysis of interest using Python in Google Colab\n",
        "Learn how and why to perform basic cleaning processes\n",
        "Learn how to perform data enrichment processes (POS tagging, chunking) and what they can add to analysis\n",
        "8. Research Phase most relevant to your lesson (delete as appropriate) Acquire / Transform / Analyze \n",
        "9. Research Area most relevant to your lesson (delete as appropriate) python / data manipulation / distant reading \n",
        "10. Intended Submission Date: October/November 2022?\n",
        "11. Lesson will use open technology and data at no cost to the reader: Yes\n",
        "12. Any other comments for the editor: N/A\n"
      ],
      "metadata": {
        "id": "PTFYiEX0iKf-"
      }
    }
  ]
}